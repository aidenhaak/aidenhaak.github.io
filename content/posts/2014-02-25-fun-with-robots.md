---
title: Fun With Robots
date: 2014-02-25
description: I've had a bit of fun recently poking around and looking at a few websites' *robots.txt* files. Yes, it has been an exciting past couple of days!
draft: false
type: post
---

I've had a bit of fun recently poking around and looking at a few websites' *robots.txt* files. Yes, it has been an exciting past couple of days!

For those of you who don't know, a [*robots.txt*](http://www.robotstxt.org/robotstxt.html "About /robots.txt") is a file that gives instructions to web robots AKA web spiders or crawlers (e.g. Google's web crawler). These instructions tell, or more accurately suggest, where the robots can and cannot access and how often they can query your website among other things.

However, the crawlers can often just ignore the your *robots.txt* suggestions like in the case of Yandex crawler from Russia or Baidu crawler from China or any malicious crawler. This can sometime drive people to [block bad bots using access rules on their web server](https://github.com/bluedragonz/bad-bot-blocker "Bad Bot Blocker").

Some of *robots.txt* files are boring like [Wikipedia's](http://en.wikipedia.org/robots.txt) but a lot of them contain Easter eggs such as [Youtube](http://www.youtube.com/robots.txt) (for those that [don't know the reference](http://www.youtube.com/watch?v=WGoi1MSGu64 "Flight Of The Conchords - The Humans Are Dead")) or [contain ASCII art](https://logotype.se/robots.txt). And [StackOverflow](http://stackoverflow.com/robots.txt) apparently doesn't like the Yahoo's bot.

There is also the less common and less well known [*humans.txt*](http://humanstxt.org/ "Humans TXT") that tells you the actual humans behind the website. Google for example makes up for a [boring](https://www.google.com/robots.txt) *robots.txt* file and have a bit of fun with their [*humans.txt*](https://www.google.com/humans.txt) and also adding a recruiting spiel similar to [Glassdoor](http://www.glassdoor.com/robots.txt). Other websites such as [Facebook](https://www.facebook.com/robots.txt), [LinkedIn](http://www.linkedin.com/robots.txt) or [Github](https://github.com/robots.txt) take an understandably dim view to unauthorized crawling.

So there you have it - a 30,000 foot overview of *robots.txt*. Now get out there hide some Easter eggs for robots (or humans!) to find.
